# Notebooks - Sistema de Recomendaci√≥n de Cultivos

Este directorio contiene los notebooks principales del proyecto de Machine Learning para la recomendaci√≥n de cultivos agr√≠colas.

---

## Notebooks

### 1. `1_crops_eda.ipynb` - An√°lisis Exploratorio de Datos (EDA)

Este notebook contiene un an√°lisis exploratorio completo del dataset de recomendaci√≥n de cultivos.

**Contenido principal:**

- **Comprensi√≥n del negocio**: Planteamiento del problema agr√≠cola y definici√≥n de m√©tricas de √©xito
- **Obtenci√≥n y comprensi√≥n de datos**: Carga del dataset con 2,200 muestras de 22 cultivos diferentes
- **An√°lisis Univariado**:
  - An√°lisis de 7 variables num√©ricas (N, P, K, temperatura, humedad, pH, precipitaci√≥n)
  - An√°lisis de la variable objetivo (22 tipos de cultivos)
  - Detecci√≥n de outliers y valores faltantes
- **An√°lisis Multivariado**:
  - Matriz de correlaci√≥n entre variables
  - Reducci√≥n dimensional (PCA y t-SNE)
  - An√°lisis de separabilidad de clases
- **Decisiones de preprocesamiento**: Estrategias para la preparaci√≥n de datos

**Hallazgos clave:**
- Dataset de alta calidad sin valores faltantes
- Perfecto balance de clases (100 muestras por cultivo)
- Outliers v√°lidos con significado agron√≥mico
- Buena separabilidad entre cultivos (Silhouette Score t-SNE = 0.528)

---

### 2. `2_crops_modeling.ipynb` - Modelado y Evaluaci√≥n

Este notebook implementa y compara diferentes algoritmos de Machine Learning para la clasificaci√≥n de cultivos.

**Contenido principal:**

- **Preparaci√≥n de datos**:
  - Divisi√≥n estratificada del dataset (80% entrenamiento, 20% prueba)
  - Transformaci√≥n y escalamiento seg√∫n el tipo de modelo
  - Encoding de la variable objetivo

- **Entrenamiento de modelos**:
  - Implementaci√≥n de 4 algoritmos diferentes
  - Evaluaci√≥n de m√©tricas (Accuracy, Precision, Recall, F1-Score)
  - An√°lisis de matrices de confusi√≥n

- **Comparaci√≥n de resultados**:
  - Ranking de modelos por desempe√±o
  - An√°lisis de fortalezas y debilidades de cada algoritmo
  - Selecci√≥n del mejor modelo

**Modelos implementados:**
1. Decision Tree (√Årbol de Decisi√≥n)
2. Random Forest (Bosque Aleatorio)
3. Logistic Regression (Regresi√≥n Log√≠stica)
4. XGBoost (Extreme Gradient Boosting)

**Resultados:**
- Mejor modelo: XGBoost (con hiperpar√°metros por defecto)
- Performance inicial: ~99.5% accuracy
- Enlace al notebook de optimizaci√≥n: `2.5_hyperparameter_optimization.ipynb`

---

### 2.5. `2.5_hyperparameter_optimization.ipynb` - Optimizaci√≥n de Hiperpar√°metros

Este notebook se enfoca en la optimizaci√≥n sistem√°tica de hiperpar√°metros de los modelos entrenados en el notebook 2, con el objetivo de mejorar el rendimiento del 99.55% al 99.8%+.

**Contenido principal:**

- **T√©cnicas de optimizaci√≥n**:
  - GridSearchCV para Logistic Regression (b√∫squeda exhaustiva)
  - RandomizedSearchCV para Decision Tree, Random Forest y XGBoost (b√∫squeda aleatoria)
  - 5-Fold Cross-Validation para scores robustos

- **Hiperpar√°metros optimizados**:
  - **Logistic Regression**: C, penalty, solver
  - **Decision Tree**: max_depth, min_samples_split, min_samples_leaf, criterion
  - **Random Forest**: n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features
  - **XGBoost**: learning_rate, max_depth, n_estimators, subsample, colsample_bytree, gamma

- **Comparaci√≥n y selecci√≥n**:
  - Comparaci√≥n de todos los modelos optimizados
  - Visualizaciones de performance
  - Selecci√≥n autom√°tica del mejor modelo
  - Guardado de modelos optimizados

**Resultados de optimizaci√≥n:**

üèÜ **Modelo ganador: XGBoost**
- CV Score: 99.60%
- Test Accuracy: 99.32%
- Test F1-Score: 99.33%

**Hiperpar√°metros optimizados de XGBoost:**
```python
n_estimators=200        # Optimizado: 100 ‚Üí 200
max_depth=5             # Optimizado: 6 ‚Üí 5
learning_rate=0.1       # Sin cambio
subsample=1.0           # Optimizado: agregado
colsample_bytree=0.7    # Optimizado: 1.0 ‚Üí 0.7
gamma=0.1               # Optimizado: 0 ‚Üí 0.1
```

**Conceptos clave explicados:**

- **GridSearchCV vs RandomizedSearchCV**: Cu√°ndo usar cada t√©cnica
- **K-Fold Cross-Validation**: Por qu√© es mejor que Hold-out simple
- **Espacio de hiperpar√°metros**: C√≥mo definir rangos de b√∫squeda
- **Validaci√≥n sin data leakage**: Uso correcto del Test Set

---

## Modelos de Machine Learning Explicados

A continuaci√≥n se presenta una explicaci√≥n did√°ctica de cada modelo utilizado en el proyecto, dise√±ada para personas que est√°n aprendiendo Machine Learning.

---

### 1. Decision Tree (√Årbol de Decisi√≥n)

#### ¬øQu√© es?

Un √°rbol de decisi√≥n es un modelo de aprendizaje supervisado que toma decisiones mediante una serie de preguntas en forma de √°rbol. Es como un diagrama de flujo que va dividiendo los datos seg√∫n caracter√≠sticas espec√≠ficas hasta llegar a una clasificaci√≥n final.

#### ¬øC√≥mo funciona?

1. **Divisi√≥n recursiva**: El algoritmo selecciona la caracter√≠stica (feature) que mejor separa los datos en cada nodo
2. **Criterio de divisi√≥n**: Usa m√©tricas como **Gini** o **Entrop√≠a** para decidir c√≥mo dividir
3. **Hojas del √°rbol**: Los nodos finales (hojas) representan la clase predicha

**Ejemplo aplicado a cultivos:**
```
¬øPotasio (K) > 50?
‚îú‚îÄ‚îÄ S√≠: ¬øTemperatura > 25¬∞?
‚îÇ   ‚îú‚îÄ‚îÄ S√≠: Predicci√≥n ‚Üí Banana
‚îÇ   ‚îî‚îÄ‚îÄ No: Predicci√≥n ‚Üí Papa
‚îî‚îÄ‚îÄ No: ¬øpH < 6.5?
    ‚îú‚îÄ‚îÄ S√≠: Predicci√≥n ‚Üí Caf√©
    ‚îî‚îÄ‚îÄ No: Predicci√≥n ‚Üí Trigo
```

#### Puntos relevantes para aprender ML

**Ventajas:**
- Muy f√°cil de interpretar y visualizar
- No requiere normalizaci√≥n de datos
- Captura relaciones no lineales
- Maneja tanto variables num√©ricas como categ√≥ricas

**Desventajas:**
- Tendencia al **overfitting** (sobreajuste): memoriza los datos de entrenamiento
- Inestable: peque√±os cambios en los datos pueden cambiar dr√°sticamente el √°rbol
- Sesgado hacia caracter√≠sticas con m√°s categor√≠as

**Cu√°ndo usarlo:**
- Cuando necesitas un modelo interpretable
- Para exploraci√≥n inicial de datos
- Cuando tienes datos categ√≥ricos y num√©ricos mezclados

---

### 2. Random Forest (Bosque Aleatorio)

#### ¬øQu√© es?

Random Forest es un **ensemble** (conjunto) de m√∫ltiples √°rboles de decisi√≥n que trabajan juntos. Es como tener un comit√© de expertos donde cada uno da su opini√≥n y la decisi√≥n final se toma por votaci√≥n mayoritaria.

#### ¬øC√≥mo funciona?

1. **Bootstrap Aggregating (Bagging)**: Crea m√∫ltiples subconjuntos aleatorios de los datos
2. **Entrenamiento paralelo**: Entrena un √°rbol de decisi√≥n independiente con cada subconjunto
3. **Random Feature Selection**: Cada √°rbol solo considera un subconjunto aleatorio de caracter√≠sticas
4. **Votaci√≥n**: Para clasificaci√≥n, la clase m√°s votada gana; para regresi√≥n, se promedia

**Ejemplo:**
```
Dataset de cultivos (2,200 muestras)
    ‚Üì
100 √°rboles entrenados con:
- √Årbol 1: 1,760 muestras aleatorias, 5 variables aleatorias
- √Årbol 2: 1,760 muestras aleatorias, 5 variables aleatorias
- ...
- √Årbol 100: 1,760 muestras aleatorias, 5 variables aleatorias
    ‚Üì
Predicci√≥n final: Votaci√≥n mayoritaria
- 68 √°rboles ‚Üí Arroz
- 25 √°rboles ‚Üí Ma√≠z
- 7 √°rboles ‚Üí Trigo
Resultado: Arroz
```

#### Puntos relevantes para aprender ML

**Ventajas:**
- **Reduce overfitting**: Al promediar muchos √°rboles, el modelo generaliza mejor
- Muy robusto: maneja outliers y datos faltantes
- Importancia de caracter√≠sticas: indica qu√© variables son m√°s relevantes
- No requiere normalizaci√≥n de datos
- Alto accuracy en la mayor√≠a de problemas

**Desventajas:**
- Menos interpretable que un solo √°rbol
- M√°s lento de entrenar que un solo √°rbol
- Requiere m√°s memoria (almacena m√∫ltiples √°rboles)

**Hiperpar√°metros clave:**
- `n_estimators`: N√∫mero de √°rboles (m√°s √°rboles = mejor rendimiento pero m√°s lento)
- `max_depth`: Profundidad m√°xima de cada √°rbol
- `max_features`: N√∫mero de caracter√≠sticas a considerar en cada divisi√≥n

**Cu√°ndo usarlo:**
- Cuando necesitas alto rendimiento con m√≠nimo esfuerzo
- Ideal para datasets con muchas caracter√≠sticas
- Excelente opci√≥n como baseline fuerte

---

### 3. Logistic Regression (Regresi√≥n Log√≠stica)

#### ¬øQu√© es?

A pesar del nombre "regresi√≥n", es un algoritmo de **clasificaci√≥n**. Predice la probabilidad de que una muestra pertenezca a una clase aplicando una funci√≥n sigmoide a una combinaci√≥n lineal de las caracter√≠sticas.

#### ¬øC√≥mo funciona?

1. **Combinaci√≥n lineal**: Calcula un puntaje como `z = w‚ÇÅ√óN + w‚ÇÇ√óP + w‚ÇÉ√óK + ... + b`
2. **Funci√≥n sigmoide**: Convierte el puntaje en una probabilidad entre 0 y 1
   ```
   Probabilidad = 1 / (1 + e^(-z))
   ```
3. **Clasificaci√≥n multiclase**: Usa estrategias como **One-vs-Rest** o **Softmax**

**Ejemplo aplicado a cultivos:**
```
Muestra: N=90, P=42, K=43, temp=20.88, humidity=82, pH=6.5, rainfall=202.94

C√°lculo para "Arroz":
z_arroz = 0.5√ó90 + 0.3√ó42 - 0.2√ó43 + ... = 2.5
P(Arroz) = 1/(1 + e^(-2.5)) = 0.92 (92%)

C√°lculo para "Ma√≠z":
z_ma√≠z = -0.3√ó90 + 0.5√ó42 + 0.1√ó43 + ... = -1.2
P(Ma√≠z) = 1/(1 + e^(1.2)) = 0.23 (23%)

Predicci√≥n: Arroz (mayor probabilidad)
```

#### Puntos relevantes para aprender ML

**Ventajas:**
- Muy r√°pido de entrenar
- Probabil√≠stico: da probabilidades de pertenencia a cada clase
- Interpretable: los coeficientes indican la importancia de cada variable
- Funciona bien con datos linealmente separables
- Pocas probabilidades de overfitting

**Desventajas:**
- Asume relaci√≥n **lineal** entre caracter√≠sticas y log-odds
- No captura interacciones complejas sin ingenier√≠a de caracter√≠sticas
- Sensible a outliers y escala de datos (requiere normalizaci√≥n)
- Bajo rendimiento si las clases no son linealmente separables

**Preprocesamiento necesario:**
- Normalizaci√≥n/estandarizaci√≥n de datos (RobustScaler, StandardScaler)
- Transformaci√≥n de variables asim√©tricas (PowerTransformer)

**Cu√°ndo usarlo:**
- Como baseline simple y r√°pido
- Cuando necesitas probabilidades interpretables
- Problemas con relaciones lineales
- Cuando tienes pocas muestras

---

### 4. XGBoost (Extreme Gradient Boosting)

#### ¬øQu√© es?

XGBoost es un algoritmo de **ensemble** basado en **boosting** que construye √°rboles de decisi√≥n de forma **secuencial**, donde cada √°rbol intenta corregir los errores del anterior. Es uno de los algoritmos m√°s potentes y ganadores de competencias de Kaggle.

#### ¬øC√≥mo funciona?

**Diferencia clave con Random Forest:**
- **Random Forest (Bagging)**: √Årboles **paralelos** independientes, votan
- **XGBoost (Boosting)**: √Årboles **secuenciales** que corrigen errores

**Proceso:**
1. **√Årbol inicial**: Predice con un modelo simple
2. **Calcular errores (residuos)**: Identifica qu√© muestras se predijeron mal
3. **Nuevo √°rbol**: Se enfoca en las muestras con mayor error
4. **Actualizar predicci√≥n**: Predicci√≥n = Predicci√≥n anterior + (learning_rate √ó Nuevo √°rbol)
5. **Repetir**: Construir 100-1000 √°rboles de forma iterativa

**Ejemplo simplificado:**
```
Iteraci√≥n 1:
  Predicci√≥n inicial: Arroz
  Error real: Deber√≠a ser Ma√≠z
  Peso de error: 1.0

Iteraci√≥n 2:
  Nuevo √°rbol aprende: "Si N > 80 y K < 40 ‚Üí Ma√≠z (peso 1.0)"
  Predicci√≥n actualizada: 0.7√óArroz + 0.3√óMa√≠z ‚Üí Ma√≠z
  Error reducido: 0.3

Iteraci√≥n 3:
  Nuevo √°rbol afina: "Si pH > 7 ‚Üí Ma√≠z (peso 0.3)"
  Error reducido: 0.1

... (100+ iteraciones)
Predicci√≥n final: Ma√≠z (con alta confianza)
```

#### Puntos relevantes para aprender ML

**Ventajas:**
- **State-of-the-art**: Uno de los mejores algoritmos para datos tabulares
- Alt√≠simo rendimiento (accuracy)
- Maneja datos faltantes autom√°ticamente
- Regularizaci√≥n incorporada (previene overfitting)
- Muy r√°pido gracias a optimizaciones paralelas
- Importancia de caracter√≠sticas

**Desventajas:**
- Muchos hiperpar√°metros (curva de aprendizaje pronunciada)
- Riesgo de overfitting si no se ajusta correctamente
- Menos interpretable que √°rboles individuales
- Requiere m√°s tiempo de ajuste (hyperparameter tuning)

**Hiperpar√°metros clave:**
- `n_estimators`: N√∫mero de √°rboles (100-1000, m√°s = mejor pero m√°s lento)
- `max_depth`: Profundidad m√°xima (3-10, controla complejidad)
- `learning_rate` (eta): Qu√© tanto aporta cada √°rbol (0.01-0.3)
  - Alto (0.3): Aprende r√°pido, riesgo de overfitting
  - Bajo (0.01): Aprende lento, m√°s robusto, requiere m√°s √°rboles
- `subsample`: Fracci√≥n de muestras para entrenar cada √°rbol (0.8)
- `colsample_bytree`: Fracci√≥n de caracter√≠sticas por √°rbol (0.8)

**Cu√°ndo usarlo:**
- Cuando necesitas el m√°ximo rendimiento
- Competencias de Machine Learning
- Datos tabulares con relaciones complejas
- Como modelo final despu√©s de explorar otros

---

## Comparaci√≥n R√°pida de Modelos

| Criterio | Decision Tree | Random Forest | Logistic Reg. | XGBoost |
|----------|---------------|---------------|---------------|---------|
| **Interpretabilidad** | Alta | Media | Alta | Media |
| **Rendimiento** | Medio | Alto | Medio | Muy Alto |
| **Velocidad entrenamiento** | R√°pida | Media | Muy R√°pida | Media |
| **Velocidad predicci√≥n** | R√°pida | Media | Muy R√°pida | R√°pida |
| **Requiere normalizaci√≥n** | No | No | S√≠ | No |
| **Maneja no-linealidad** | S√≠ | S√≠ | No | S√≠ |
| **Riesgo de overfitting** | Alto | Bajo | Muy Bajo | Medio |
| **Tama√±o dataset ideal** | Peque√±o-Medio | Medio-Grande | Peque√±o-Grande | Medio-Grande |

---

## Flujo de Trabajo T√≠pico en ML

1. **Exploraci√≥n (EDA)** ‚Üí `1_crops_eda.ipynb`
   - Entender los datos
   - Detectar problemas
   - Decidir preprocesamiento

2. **Baseline simple** ‚Üí Logistic Regression
   - Modelo r√°pido y f√°cil
   - Establece punto de comparaci√≥n

3. **Modelos intermedios** ‚Üí Decision Tree, Random Forest
   - Explorar rendimiento sin mucho tuning

4. **Modelos avanzados** ‚Üí XGBoost
   - M√°ximo rendimiento
   - Requiere ajuste cuidadoso

5. **Evaluaci√≥n y selecci√≥n** ‚Üí `2_crops_modeling.ipynb`
   - Comparar m√©tricas
   - Analizar matrices de confusi√≥n
   - Seleccionar modelo final

---

## M√©tricas de Evaluaci√≥n

Para un problema de clasificaci√≥n multiclase (22 cultivos), las m√©tricas clave son:

- **Accuracy**: Porcentaje de predicciones correctas
  - F√≥rmula: (Predicciones Correctas) / (Total de Predicciones)
  - Interpretaci√≥n: 95% accuracy = 95 de cada 100 predicciones son correctas

- **Precision (por clase)**: De lo que predije como "Arroz", ¬øcu√°nto fue realmente Arroz?
  - F√≥rmula: Verdaderos Positivos / (Verdaderos Positivos + Falsos Positivos)
  - Importante para evitar recomendar cultivos inadecuados (costo econ√≥mico)

- **Recall (por clase)**: De todos los "Arroz" reales, ¬øcu√°ntos detect√©?
  - F√≥rmula: Verdaderos Positivos / (Verdaderos Positivos + Falsos Negativos)
  - Importante para no perder oportunidades de cultivos viables

- **F1-Score**: Promedio arm√≥nico de Precision y Recall
  - F√≥rmula: 2 √ó (Precision √ó Recall) / (Precision + Recall)
  - Balancea ambas m√©tricas

- **Matriz de Confusi√≥n**: Muestra qu√© cultivos se confunden entre s√≠
  - √ötil para identificar pares de cultivos problem√°ticos

---

## Recursos Adicionales

Para profundizar en Machine Learning, se recomiendan:

- **Scikit-learn Documentation**: https://scikit-learn.org/
- **XGBoost Documentation**: https://xgboost.readthedocs.io/
- **Curso**: "Machine Learning" de Andrew Ng (Coursera)
- **Libro**: "Hands-On Machine Learning" de Aur√©lien G√©ron
- **Kaggle**: Plataforma para practicar con datasets reales

---

## Estructura del Proyecto

```
notebooks/
‚îú‚îÄ‚îÄ 1_crops_eda.ipynb                      # An√°lisis Exploratorio de Datos
‚îú‚îÄ‚îÄ 2_crops_modeling.ipynb                 # Modelado y Evaluaci√≥n (modelos base)
‚îú‚îÄ‚îÄ 2.5_hyperparameter_optimization.ipynb  # Optimizaci√≥n de Hiperpar√°metros
‚îî‚îÄ‚îÄ README.md                              # Este archivo
```

## Flujo de Trabajo del Proyecto

```mermaid
graph TD
    A[1_crops_eda.ipynb<br/>An√°lisis Exploratorio] --> B[Comprensi√≥n de datos<br/>Calidad del dataset<br/>Decisiones de preprocesamiento]

    B --> C[2_crops_modeling.ipynb<br/>Modelado Base]

    C --> D[Modelos con hiperpar√°metros<br/>por defecto]
    D --> E[Comparaci√≥n inicial<br/>Performance ~99.5%]

    E --> F[2.5_hyperparameter_optimization.ipynb<br/>Optimizaci√≥n]

    F --> G[GridSearchCV<br/>Logistic Regression]
    F --> H[RandomizedSearchCV<br/>Tree, RF, XGBoost]

    G --> I[5-Fold Cross-Validation]
    H --> I

    I --> J[üèÜ XGBoost Optimizado<br/>CV: 99.60%<br/>Test: 99.32%]

    J --> K[3_crops_feature_engineering.ipynb<br/>Pr√≥ximo paso]

    K --> L[Nuevas features:<br/>interacciones, ratios]
    L --> M[SHAP Analysis]
    M --> N[Modelo Final<br/>Objetivo: 99.8%+]

    style A fill:#e1f5ff
    style C fill:#e1f5ff
    style F fill:#fff4e1
    style J fill:#d4edda
    style K fill:#f8d7da
    style N fill:#d4edda
```

---

**Proyecto desarrollado para el curso de Machine Learning Aplicado - EAFIT University**

**Integrantes:**
- Daniel Alejandro Garcia Zuluaica
- Edward Alejandro Rayo Cort√©s
- Elizabeth Toro Chalarca

